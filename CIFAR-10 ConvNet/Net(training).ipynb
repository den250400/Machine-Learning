{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Denis\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "Epoch 1/30\n",
      "390/390 [==============================] - 105s 269ms/step - loss: 0.2028 - acc: 0.9888 - val_loss: 0.6674 - val_acc: 0.9043\n",
      "Epoch 2/30\n",
      "390/390 [==============================] - 101s 259ms/step - loss: 0.1974 - acc: 0.9903 - val_loss: 0.6627 - val_acc: 0.9079\n",
      "Epoch 3/30\n",
      "390/390 [==============================] - 101s 260ms/step - loss: 0.1953 - acc: 0.9905 - val_loss: 0.7085 - val_acc: 0.8990\n",
      "Epoch 4/30\n",
      "390/390 [==============================] - 101s 260ms/step - loss: 0.1939 - acc: 0.9909 - val_loss: 0.6448 - val_acc: 0.9069\n",
      "Epoch 5/30\n",
      "390/390 [==============================] - 102s 261ms/step - loss: 0.1922 - acc: 0.9901 - val_loss: 0.6092 - val_acc: 0.9124\n",
      "Epoch 6/30\n",
      "390/390 [==============================] - 102s 261ms/step - loss: 0.1935 - acc: 0.9903 - val_loss: 0.6620 - val_acc: 0.9037\n",
      "Epoch 7/30\n",
      "390/390 [==============================] - 102s 260ms/step - loss: 0.1908 - acc: 0.9902 - val_loss: 0.6429 - val_acc: 0.9076\n",
      "Epoch 8/30\n",
      "390/390 [==============================] - 102s 261ms/step - loss: 0.1888 - acc: 0.9911 - val_loss: 0.6417 - val_acc: 0.9064\n",
      "Epoch 9/30\n",
      "390/390 [==============================] - 102s 261ms/step - loss: 0.1868 - acc: 0.9911 - val_loss: 0.6675 - val_acc: 0.9051\n",
      "Epoch 10/30\n",
      "390/390 [==============================] - 102s 260ms/step - loss: 0.1854 - acc: 0.9914 - val_loss: 0.6227 - val_acc: 0.9098\n",
      "Epoch 11/30\n",
      "390/390 [==============================] - 102s 261ms/step - loss: 0.1852 - acc: 0.9908 - val_loss: 0.6550 - val_acc: 0.9031\n",
      "Epoch 12/30\n",
      "390/390 [==============================] - 101s 260ms/step - loss: 0.1828 - acc: 0.9914 - val_loss: 0.6389 - val_acc: 0.9069\n",
      "Epoch 13/30\n",
      "390/390 [==============================] - 101s 260ms/step - loss: 0.1821 - acc: 0.9915 - val_loss: 0.6902 - val_acc: 0.8996\n",
      "Epoch 14/30\n",
      "390/390 [==============================] - 102s 261ms/step - loss: 0.1842 - acc: 0.9906 - val_loss: 0.6506 - val_acc: 0.9057\n",
      "Epoch 15/30\n",
      "390/390 [==============================] - 101s 260ms/step - loss: 0.1820 - acc: 0.9910 - val_loss: 0.6700 - val_acc: 0.9045\n",
      "Epoch 16/30\n",
      "390/390 [==============================] - 101s 260ms/step - loss: 0.1815 - acc: 0.9909 - val_loss: 0.6958 - val_acc: 0.8988\n",
      "Epoch 17/30\n",
      "390/390 [==============================] - 101s 259ms/step - loss: 0.1777 - acc: 0.9917 - val_loss: 0.6664 - val_acc: 0.9018\n",
      "Epoch 18/30\n",
      "390/390 [==============================] - 101s 259ms/step - loss: 0.1840 - acc: 0.9898 - val_loss: 0.6705 - val_acc: 0.9001\n",
      "Epoch 19/30\n",
      "390/390 [==============================] - 101s 259ms/step - loss: 0.1797 - acc: 0.9906 - val_loss: 0.6159 - val_acc: 0.9046\n",
      "Epoch 20/30\n",
      "390/390 [==============================] - 101s 260ms/step - loss: 0.1781 - acc: 0.9919 - val_loss: 0.6280 - val_acc: 0.9077\n",
      "Epoch 21/30\n",
      "390/390 [==============================] - 101s 259ms/step - loss: 0.1770 - acc: 0.9912 - val_loss: 0.6041 - val_acc: 0.9118\n",
      "Epoch 22/30\n",
      "390/390 [==============================] - 101s 259ms/step - loss: 0.1775 - acc: 0.9909 - val_loss: 0.6363 - val_acc: 0.9071\n",
      "Epoch 23/30\n",
      "390/390 [==============================] - 101s 259ms/step - loss: 0.1766 - acc: 0.9913 - val_loss: 0.6261 - val_acc: 0.9053\n",
      "Epoch 24/30\n",
      "390/390 [==============================] - 101s 259ms/step - loss: 0.1762 - acc: 0.9906 - val_loss: 0.6326 - val_acc: 0.9055\n",
      "Epoch 25/30\n",
      "390/390 [==============================] - 101s 259ms/step - loss: 0.1705 - acc: 0.9924 - val_loss: 0.6698 - val_acc: 0.9011\n",
      "Epoch 26/30\n",
      "390/390 [==============================] - 101s 259ms/step - loss: 0.1746 - acc: 0.9905 - val_loss: 0.6548 - val_acc: 0.9039\n",
      "Epoch 27/30\n",
      "390/390 [==============================] - 101s 259ms/step - loss: 0.1733 - acc: 0.9909 - val_loss: 0.5986 - val_acc: 0.9137\n",
      "Epoch 28/30\n",
      "390/390 [==============================] - 101s 259ms/step - loss: 0.1724 - acc: 0.9917 - val_loss: 0.6501 - val_acc: 0.9026\n",
      "Epoch 29/30\n",
      "390/390 [==============================] - 101s 259ms/step - loss: 0.1712 - acc: 0.9911 - val_loss: 0.6047 - val_acc: 0.9079\n",
      "Epoch 30/30\n",
      "390/390 [==============================] - 101s 259ms/step - loss: 0.1727 - acc: 0.9909 - val_loss: 0.6135 - val_acc: 0.9068\n",
      "10000/10000 [==============================] - 9s 923us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.datasets import cifar10 # subroutines for fetching the CIFAR-10 dataset\n",
    "from keras.models import Model, Sequential # basic class for specifying and training a neural network\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Dense, Dropout, Flatten, BatchNormalization, advanced_activations\n",
    "from keras.layers.advanced_activations import PReLU, ELU\n",
    "from keras.utils import np_utils # utilities for one-hot encoding of ground truth values\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#Training parameters\n",
    "batch_size = 128\n",
    "num_epochs = 30\n",
    "weight_decay = 1e-4\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "num_train, height, width, depth = X_train.shape # there are 50000 training examples in CIFAR-10 \n",
    "input_shape = (height, width, depth)\n",
    "num_test = X_test.shape[0] # there are 10000 test examples in CIFAR-10\n",
    "num_classes = np.unique(y_train).shape[0] # there are 10 image classes\n",
    "\n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255 # Normalise data to [0, 1] range\n",
    "X_test /= 255 # Normalise data to [0, 1] range\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, num_classes) # One-hot encode the labels\n",
    "Y_test = np_utils.to_categorical(y_test, num_classes) # One-hot encode the labels\n",
    "\n",
    "#Defining data augmentation operation\n",
    "datagen = ImageDataGenerator(featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False)\n",
    "datagen.fit(X_train)\n",
    "\n",
    "model = Sequential()\n",
    "#Convolutional layer 1\n",
    "model.add(Conv2D(64, kernel_size=(5, 5), strides=(1, 1), activation='linear', padding='same', input_shape=input_shape, kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(PReLU(alpha_initializer='zeros', weights=None))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, kernel_size=(5, 5), strides=(1, 1), activation='linear', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(PReLU(alpha_initializer='zeros', weights=None))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "#Convolutional layer 2\n",
    "model.add(Conv2D(128, kernel_size=(5, 5), strides=(1, 1), activation='linear', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(PReLU(alpha_initializer='zeros', weights=None))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, kernel_size=(5, 5), strides=(1, 1), activation='linear', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(PReLU(alpha_initializer='zeros', weights=None))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, kernel_size=(5, 5), strides=(1, 1), activation='linear', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(PReLU(alpha_initializer='zeros', weights=None))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "#Convolutional layer 3\n",
    "model.add(Conv2D(256, kernel_size=(5, 5), strides=(1, 1), activation='linear', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(PReLU(alpha_initializer='zeros', weights=None))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, kernel_size=(5, 5), strides=(1, 1), activation='linear', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(PReLU(alpha_initializer='zeros', weights=None))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, kernel_size=(5, 5), strides=(1, 1), activation='linear', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(PReLU(alpha_initializer='zeros', weights=None))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#Fully-connected layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='linear'))\n",
    "model.add(PReLU(alpha_initializer='zeros', weights=None))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "model.load_weights('Model/modelv6.h5')\n",
    "#model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs, verbose=1, validation_split=0.1)\n",
    "model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size), steps_per_epoch=len(X_train) // batch_size, epochs=num_epochs, verbose=1, validation_data=(X_test,Y_test), callbacks=[tbCallBack])\n",
    "model.evaluate(X_test, Y_test, verbose=1)\n",
    "model.save('Model/modelv6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
